import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from modelscope import snapshot_download
import json
import os
import pandas as pd
from tqdm import tqdm
import numpy as np
import matplotlib.pyplot as plt
from rouge import Rouge
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

# ================= 1. é…ç½®åŒºåŸŸ =================
ROOT_DIR = "/opt/data/private/qwen3_train"
OUTPUT_DIR = os.path.join(ROOT_DIR, "output")
MODEL_CACHE_DIR = os.path.join(ROOT_DIR, "model_cache")
DATA_PATH = os.path.join(ROOT_DIR, "interview_data.json")
MODEL_ID = "Qwen/Qwen3-8B"

# ã€é‡è¦ã€‘æµ‹è¯•æ ·æœ¬æ•°é™åˆ¶
# è®¾ä¸º 30 å¤§çº¦è·‘ 10åˆ†é’Ÿï¼›è®¾ä¸º None åˆ™è·‘å®Œæ‰€æœ‰æ•°æ®(çº¦1å°æ—¶)
TEST_SAMPLE_NUM = None

# System Prompt
SYSTEM_PROMPT = "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è®¡ç®—æœºä¸“ä¸šé¢è¯•å®˜ï¼Œé£æ ¼ä¸¥è°¨ï¼Œå–œæ¬¢è¿½é—®åº•å±‚åŸç†ã€‚è¯·æ ¹æ®å€™é€‰äººçš„å›ç­”è¿›è¡Œè¿½é—®æˆ–ç‚¹è¯„ã€‚é¢è¯•ä¸­å¯¹è¯ä¸è¶…è¿‡10è½®ï¼Œå®Œæˆé¢è¯•æ—¶é¢è¯•å®˜ä¸»åŠ¨ç»“æŸå¹¶ç»™å‡ºæ‰“åˆ†å’Œç‚¹è¯„ã€‚"


# ==============================================

def get_latest_checkpoint(output_dir):
    if not os.path.exists(output_dir): return None
    checkpoints = [d for d in os.listdir(output_dir) if d.startswith("checkpoint-")]
    if not checkpoints: return None
    checkpoints.sort(key=lambda x: int(x.split("-")[-1]))
    return os.path.join(output_dir, checkpoints[-1])


def load_model_and_tokenizer():
    print("â³ æ­£åœ¨åŠ è½½æ¨¡å‹ (è¿™åªéœ€è¦åŠ è½½ä¸€æ¬¡)...")
    try:
        model_dir = snapshot_download(MODEL_ID, cache_dir=MODEL_CACHE_DIR, revision="master")
    except:
        model_dir = MODEL_ID

    tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)

    # 1. åŠ è½½åŸºåº§æ¨¡å‹
    base_model = AutoModelForCausalLM.from_pretrained(
        model_dir,
        device_map="auto",
        torch_dtype=torch.bfloat16,
        trust_remote_code=True
    )

    # 2. æŒ‚è½½ LoRA
    lora_path = get_latest_checkpoint(OUTPUT_DIR)
    if not lora_path:
        raise FileNotFoundError("âŒ æœªæ‰¾åˆ° output ä¸‹çš„ checkpointï¼Œè¯·å…ˆè¿è¡Œ train.py")

    print(f"âœ… æŒ‚è½½ LoRA æƒé‡: {lora_path}")
    model = PeftModel.from_pretrained(base_model, lora_path)
    model.eval()

    return model, tokenizer


def predict(model, tokenizer, history):
    text = tokenizer.apply_chat_template(history, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer([text], return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            inputs.input_ids,
            max_new_tokens=512,
            temperature=0.7,
            top_p=0.9
        )
    generated_ids = [out[len(inp):] for inp, out in zip(inputs.input_ids, outputs)]
    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)


def calculate_metrics(predictions, references):
    rouge = Rouge()
    smooth = SmoothingFunction().method1
    bleu_scores = []
    rouge_l_scores = []

    for pred, ref in zip(predictions, references):
        pred_tokens = list(pred) if pred else [" "]
        ref_tokens = list(ref) if ref else [" "]

        if not pred.strip(): pred = " "

        # BLEU-4
        score = sentence_bleu([ref_tokens], pred_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth)
        bleu_scores.append(score)

        # ROUGE-L
        try:
            scores = rouge.get_scores(" ".join(pred_tokens), " ".join(ref_tokens))
            rouge_l_scores.append(scores[0]['rouge-l']['f'])
        except:
            rouge_l_scores.append(0.0)

    return {
        "BLEU-4": np.mean(bleu_scores),
        "ROUGE-L": np.mean(rouge_l_scores)
    }


def plot_comparison(base_metrics, sft_metrics, save_path):
    labels = ['BLEU-4', 'ROUGE-L']
    base_scores = [base_metrics['BLEU-4'], base_metrics['ROUGE-L']]
    sft_scores = [sft_metrics['BLEU-4'], sft_metrics['ROUGE-L']]

    x = np.arange(len(labels))
    width = 0.35

    plt.figure(figsize=(8, 6))
    rects1 = plt.bar(x - width / 2, base_scores, width, label='Base Model', color='#d3d3d3')
    rects2 = plt.bar(x + width / 2, sft_scores, width, label='Fine-tuned (Ours)', color='#4e79a7')

    plt.ylabel('Score')
    plt.title('Performance Comparison: Base vs Fine-tuned')
    plt.xticks(x, labels)
    plt.ylim(0, 1.0)
    plt.legend()

    plt.bar_label(rects1, padding=3, fmt='%.2f')
    plt.bar_label(rects2, padding=3, fmt='%.2f')

    plt.savefig(save_path, dpi=300)
    print(f"ğŸ“Š å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜: {save_path}")


def main():
    # 1. å‡†å¤‡æ•°æ®
    with open(DATA_PATH, 'r', encoding='utf-8') as f:
        data = json.load(f)
    test_data = data[int(len(data) * 0.9):]  # å–å10%

    if TEST_SAMPLE_NUM and len(test_data) > TEST_SAMPLE_NUM:
        print(f"âš ï¸ ä»…æˆªå–å‰ {TEST_SAMPLE_NUM} æ¡æ•°æ®è¿›è¡Œå¿«é€Ÿå¯¹æ¯”...")
        test_data = test_data[:TEST_SAMPLE_NUM]

    model, tokenizer = load_model_and_tokenizer()

    results = []
    preds_base = []
    preds_sft = []
    ground_truths = []

    print("ğŸš€ å¼€å§‹åŒæ¨¡å‹æ¨ç†å¯¹æ¯”...")
    for item in tqdm(test_data):
        convs = item['conversations']

        last_human_idx = -1
        for i, msg in enumerate(convs):
            if msg['from'] == 'human':
                last_human_idx = i

        if last_human_idx == -1: continue

        history = [{"role": "system", "content": SYSTEM_PROMPT}]
        for i in range(last_human_idx + 1):
            role = "user" if convs[i]['from'] == "human" else "assistant"
            history.append({"role": role, "content": convs[i]['value']})

        ground_truth = convs[last_human_idx + 1]['value']
        ground_truths.append(ground_truth)

        # --- æ ¸å¿ƒé€»è¾‘ï¼šåˆ†åˆ«æ¨ç† ---

        # 1. Base Model (ä¸´æ—¶ç¦ç”¨ Adapter)
        with model.disable_adapter():
            res_base = predict(model, tokenizer, history)
            preds_base.append(res_base)

        # 2. SFT Model (æ­£å¸¸å¯ç”¨ Adapter)
        res_sft = predict(model, tokenizer, history)
        preds_sft.append(res_sft)

        results.append({
            "User Query": history[-1]['content'],
            "Ground Truth": ground_truth,
            "Base Prediction": res_base,
            "SFT Prediction": res_sft
        })

    # 2. è®¡ç®—æŒ‡æ ‡
    print("ğŸ“ˆ æ­£åœ¨è®¡ç®—æŒ‡æ ‡...")
    metrics_base = calculate_metrics(preds_base, ground_truths)
    metrics_sft = calculate_metrics(preds_sft, ground_truths)

    print("\n" + "=" * 45)
    print(f"{'Metric':<15} | {'Base Model':<12} | {'SFT Model':<12}")
    print("-" * 45)
    print(f"{'BLEU-4':<15} | {metrics_base['BLEU-4']:.4f}       | {metrics_sft['BLEU-4']:.4f}")
    print(f"{'ROUGE-L':<15} | {metrics_base['ROUGE-L']:.4f}       | {metrics_sft['ROUGE-L']:.4f}")
    print("=" * 45 + "\n")

    # 3. ä¿å­˜ç»“æœ
    plot_comparison(metrics_base, metrics_sft, os.path.join(ROOT_DIR, "comparison_chart.png"))

    # ä¿å­˜ Excel (éœ€è¦ openpyxl)
    df = pd.DataFrame(results)
    excel_path = os.path.join(ROOT_DIR, "comparison_results.xlsx")
    try:
        df.to_excel(excel_path, index=False)
        print(f"ğŸ’¾ è¯¦ç»†å¯¹æ¯”æ•°æ®å·²ä¿å­˜è‡³: {excel_path}")
    except ModuleNotFoundError:
        print("âŒ é”™è¯¯: æœªå®‰è£… openpyxlï¼Œæ— æ³•ä¿å­˜ä¸º Excelã€‚")
        print("ğŸ’¡ è¯·è¿è¡Œ: pip install openpyxl")


if __name__ == "__main__":
    main()