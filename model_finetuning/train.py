import json
import pandas as pd
import torch
from datasets import Dataset
from modelscope import snapshot_download, AutoTokenizer
from transformers import AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq, TrainerCallback # ã€ä¿®æ”¹1ã€‘å¼•å…¥ TrainerCallback
from peft import LoraConfig, TaskType, get_peft_model
import os
import swanlab
import matplotlib.pyplot as plt

# ================== 1. æ ¸å¿ƒé…ç½®åŒºåŸŸ ==================

# ã€Aã€‘è·¯å¾„é…ç½®
ROOT_DIR = "/opt/data/private/qwen3_train"
DATA_PATH = os.path.join(ROOT_DIR, "interview_data.json")
MODEL_CACHE_DIR = os.path.join(ROOT_DIR, "model_cache")
OUTPUT_DIR = os.path.join(ROOT_DIR, "output")
SWANLAB_DIR = os.path.join(ROOT_DIR, "swanlog")

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(MODEL_CACHE_DIR, exist_ok=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(SWANLAB_DIR, exist_ok=True)

# ã€Bã€‘æ¨¡å‹ä¸Prompté…ç½®
MODEL_ID = "Qwen/Qwen3-8B"
MAX_LENGTH = 4096

# å®šä¹‰å…¨å±€ç»Ÿä¸€çš„ System Prompt
SYSTEM_PROMPT = "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è®¡ç®—æœºä¸“ä¸šé¢è¯•å®˜ï¼Œé£æ ¼ä¸¥è°¨ï¼Œå–œæ¬¢è¿½é—®åº•å±‚åŸç†ã€‚è¯·æ ¹æ®å€™é€‰äººçš„å›ç­”è¿›è¡Œè¿½é—®æˆ–ç‚¹è¯„ã€‚é¢è¯•ä¸­å¯¹è¯ä¸è¶…è¿‡10è½®ï¼Œå®Œæˆé¢è¯•æ—¶é¢è¯•å®˜ä¸»åŠ¨ç»“æŸå¹¶ç»™å‡ºæ‰“åˆ†å’Œç‚¹è¯„ã€‚"

# åˆå§‹åŒ– SwanLab
os.environ["SWANLAB_PROJECT"] = "qwen3-8b-interview-sft"
swanlab.init(project=os.environ["SWANLAB_PROJECT"], mode="local", logdir=SWANLAB_DIR)

swanlab.config.update({
    "model": MODEL_ID,
    "data_path": DATA_PATH,
    "data_max_length": MAX_LENGTH,
    "method": "LoRA",
    "task": "Interviewer Simulation",
    "system_prompt": SYSTEM_PROMPT
})


# ================== 2. æ•°æ®å¤„ç†å‡½æ•° ==================
def process_func(example):
    input_ids = []
    labels = []

    conversation = example["conversations"]

    if conversation[0]["from"] != "system":
        system_head = tokenizer.encode("<|im_start|>system\n", add_special_tokens=False)
        system_content = tokenizer.encode(SYSTEM_PROMPT, add_special_tokens=False)
        system_tail = tokenizer.encode("<|im_end|>\n", add_special_tokens=False)

        input_ids += system_head + system_content + system_tail
        labels += [-100] * (len(system_head) + len(system_content) + len(system_tail))

    role_map = {"system": "system", "human": "user", "gpt": "assistant"}

    for message in conversation:
        role = message["from"]
        content = message["value"]
        qwen_role = role_map.get(role, "user")

        head_text = f"<|im_start|>{qwen_role}\n"
        head_ids = tokenizer.encode(head_text, add_special_tokens=False)
        content_ids = tokenizer.encode(content, add_special_tokens=False)
        tail_text = "<|im_end|>\n"
        tail_ids = tokenizer.encode(tail_text, add_special_tokens=False)

        current_ids = head_ids + content_ids + tail_ids
        input_ids.extend(current_ids)

        if role == "gpt":
            current_labels = [-100] * len(head_ids) + content_ids + tail_ids
        else:
            current_labels = [-100] * len(current_ids)

        labels.extend(current_labels)

    if len(input_ids) > MAX_LENGTH:
        input_ids = input_ids[:MAX_LENGTH]
        labels = labels[:MAX_LENGTH]

    attention_mask = [1] * len(input_ids)
    return {"input_ids": input_ids, "attention_mask": attention_mask, "labels": labels}


def predict(messages, model, tokenizer):
    device = "cuda"
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    model_inputs = tokenizer([text], return_tensors="pt").to(device)

    generated_ids = model.generate(
        model_inputs.input_ids,
        max_new_tokens=1024,
        temperature=0.7,
        top_p=0.9
    )
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    return response


# ================== 3. æ¨¡å‹ä¸‹è½½ä¸åŠ è½½ ==================
print(f"æ­£åœ¨ä¸‹è½½/åŠ è½½æ¨¡å‹: {MODEL_ID}...")
print(f"ç¼“å­˜ç›®å½•: {MODEL_CACHE_DIR}")

try:
    model_dir = snapshot_download(MODEL_ID, cache_dir=MODEL_CACHE_DIR, revision="master")
except Exception as e:
    print(f"ä¸‹è½½æŠ¥é”™: {e}")
    raise e

tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=False, trust_remote_code=True)

model = AutoModelForCausalLM.from_pretrained(
    model_dir,
    device_map="auto",
    torch_dtype=torch.bfloat16,
    trust_remote_code=True
)

try:
    if model.generation_config:
        setattr(model.generation_config, "enable_thinking", True)
except Exception:
    pass

model.enable_input_require_grads()

# ================== 4. LoRA é…ç½® ==================
config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    inference_mode=False,
    r=32,
    lora_alpha=64,
    lora_dropout=0.1
)
model = get_peft_model(model, config)
model.print_trainable_parameters()

# ================== 5. æ•°æ®é›†åŠ è½½ ==================
print(f"æ­£åœ¨è¯»å–æ•°æ®: {DATA_PATH}")
if not os.path.exists(DATA_PATH):
    raise FileNotFoundError(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {DATA_PATH}ï¼Œè¯·ç¡®è®¤æ–‡ä»¶åæ­£ç¡®ï¼")

with open(DATA_PATH, 'r', encoding='utf-8') as f:
    data_list = json.load(f)

full_ds = Dataset.from_list(data_list)
split_ds = full_ds.train_test_split(test_size=0.1, seed=42)
train_ds = split_ds["train"]
eval_ds = split_ds["test"]

train_dataset = train_ds.map(process_func, remove_columns=train_ds.column_names)
eval_dataset = eval_ds.map(process_func, remove_columns=eval_ds.column_names)

# ================== ã€ä¿®æ”¹2ã€‘å®šä¹‰å›è°ƒå‡½æ•° ==================
class EvalAtStep10Callback(TrainerCallback):
    """
    è‡ªå®šä¹‰å›è°ƒï¼šä»…åœ¨ç¬¬ 10 æ­¥æ—¶å¼ºåˆ¶è§¦å‘ä¸€æ¬¡è¯„ä¼°
    """
    def on_step_end(self, args, state, control, **kwargs):
        if state.global_step == 10:
            control.should_evaluate = True

# ================== 6. è®­ç»ƒå‚æ•° ==================
args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=8,
    eval_strategy="steps",
    eval_steps=50,
    logging_steps=10,
    num_train_epochs=2,  # ã€ä¿®æ”¹3ã€‘æ”¹ä¸º 2 è½®
    save_steps=100,
    learning_rate=1e-4,
    save_on_each_node=True,
    gradient_checkpointing=True,
    report_to="swanlab",
    run_name="qwen3-interview",
    bf16=True,
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),
    callbacks=[EvalAtStep10Callback()] # ã€ä¿®æ”¹4ã€‘æ³¨å†Œå›è°ƒ
)

print("ğŸš€ å¼€å§‹è®­ç»ƒé¢è¯•å®˜æ¨¡å‹...")
trainer.train()

# ================== ã€Loss ç»˜å›¾éƒ¨åˆ† (æ— éœ€ä¿®æ”¹ï¼Œä¼šè‡ªåŠ¨è¯»å–)ã€‘ ==================
print("ğŸ“Š æ­£åœ¨ç”Ÿæˆ Loss æ›²çº¿å›¾...")

# æå–æ—¥å¿—å†å²
log_history = trainer.state.log_history

# åˆ†ç¦»è®­ç»ƒ loss å’ŒéªŒè¯ loss
train_steps = []
train_loss = []
eval_steps = []
eval_loss = []

for log in log_history:
    if "loss" in log and "step" in log:
        train_steps.append(log["step"])
        train_loss.append(log["loss"])
    if "eval_loss" in log and "step" in log:
        eval_steps.append(log["step"])
        eval_loss.append(log["eval_loss"])

# ç»˜å›¾
plt.figure(figsize=(10, 6))

if train_steps:
    plt.plot(train_steps, train_loss, label="Training Loss", alpha=0.7, color="blue")

if eval_steps:
    plt.plot(eval_steps, eval_loss, label="Evaluation Loss", marker='o', color="red", linestyle="--")

plt.xlabel("Global Steps")
plt.ylabel("Loss")
plt.title("Training and Evaluation Loss Curve")
plt.legend()
plt.grid(True, linestyle='--', alpha=0.5)

loss_plot_path = os.path.join(OUTPUT_DIR, "loss_curve.png")
plt.savefig(loss_plot_path)
print(f"âœ… Loss æ›²çº¿å·²ä¿å­˜è‡³: {loss_plot_path}")
plt.close()

# ================== 7. è®­ç»ƒåæµ‹è¯• ==================
print("=== å¼€å§‹æ¨¡æ‹Ÿé¢è¯•æµ‹è¯• ===")
test_samples = eval_ds.select(range(min(2, len(eval_ds))))
test_text_list = []

for sample in test_samples:
    conversations = sample['conversations']
    input_messages = []
    ground_truth = ""
    last_human_idx = -1

    for i, msg in enumerate(conversations):
        if msg['from'] == 'human':
            last_human_idx = i
        elif msg['from'] == 'gpt' and i > last_human_idx:
            ground_truth = msg['value']

    input_messages.append({"role": "system", "content": SYSTEM_PROMPT})

    for i in range(last_human_idx + 1):
        msg = conversations[i]
        role = "user" if msg['from'] == "human" else "assistant"
        input_messages.append({"role": role, "content": msg['value']})

    response = predict(input_messages, model, tokenizer)
    last_user_input = input_messages[-1]['content']

    log_text = f"""
    ã€Context Systemã€‘: {input_messages[0]['content']}
    ã€Candidate Answerã€‘: {last_user_input}
    ã€Real Interviewerã€‘: {ground_truth}
    ã€AI Interviewerã€‘: {response}
    """

    test_text_list.append(swanlab.Text(log_text))
    print(log_text)
    print("-" * 50)

swanlab.log({"Interview_Prediction": test_text_list})
swanlab.finish()