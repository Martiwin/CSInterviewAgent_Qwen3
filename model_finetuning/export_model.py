import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from modelscope import snapshot_download
import os
import shutil

# ================= é…ç½®åŒºåŸŸ =================
ROOT_DIR = "/opt/data/private/qwen3_train"
MODEL_CACHE_DIR = os.path.join(ROOT_DIR, "model_cache")
OUTPUT_DIR = os.path.join(ROOT_DIR, "output")
MERGED_DIR = os.path.join(ROOT_DIR, "qwen3_interview_merged") # åˆå¹¶åå­˜æ”¾çš„ä¸´æ—¶ç›®å½•
MODEL_ID = "Qwen/Qwen3-8B"
# ===========================================

def get_latest_checkpoint(output_dir):
    if not os.path.exists(output_dir): return None
    checkpoints = [d for d in os.listdir(output_dir) if d.startswith("checkpoint-")]
    if not checkpoints: return None
    checkpoints.sort(key=lambda x: int(x.split("-")[-1]))
    return os.path.join(output_dir, checkpoints[-1])

# 1. è·å–è·¯å¾„
print("ğŸ” æ­£åœ¨å®šä½è·¯å¾„...")
base_model_path = snapshot_download(MODEL_ID, cache_dir=MODEL_CACHE_DIR, revision="master")
lora_path = get_latest_checkpoint(OUTPUT_DIR)
print(f"âœ… åŸºç¡€æ¨¡å‹: {base_model_path}")
print(f"âœ… LoRAæƒé‡: {lora_path}")

# 2. åŠ è½½å¹¶åˆå¹¶
print("â³ æ­£åœ¨åŠ è½½å¹¶åˆå¹¶æ¨¡å‹ (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...")
tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_path,
    torch_dtype=torch.float16, # è½¬æ¢ GGUF å»ºè®®å…ˆè½¬ä¸º fp16
    device_map="auto",
    trust_remote_code=True
)

model = PeftModel.from_pretrained(base_model, lora_path)
model = model.merge_and_unload() # æ ¸å¿ƒï¼šèåˆæƒé‡

# 3. ä¿å­˜
print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜åˆå¹¶åçš„æ¨¡å‹åˆ°: {MERGED_DIR}")
if os.path.exists(MERGED_DIR):
    shutil.rmtree(MERGED_DIR) # æ¸…ç†æ—§æ–‡ä»¶
model.save_pretrained(MERGED_DIR)
tokenizer.save_pretrained(MERGED_DIR)

print("ğŸ‰ åˆå¹¶å®Œæˆï¼å‡†å¤‡è¿›è¡Œæ ¼å¼è½¬æ¢ã€‚")