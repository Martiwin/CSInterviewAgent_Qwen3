import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
from peft import PeftModel
from threading import Thread
import os
from modelscope import snapshot_download

# ================= é…ç½®åŒºåŸŸ (ä¿æŒä¸ train.py ä¸€è‡´) =================
ROOT_DIR = "/opt/data/private/qwen3_train"
OUTPUT_DIR = os.path.join(ROOT_DIR, "output")
MODEL_CACHE_DIR = os.path.join(ROOT_DIR, "model_cache")
MODEL_ID = "Qwen/Qwen3-8B"

# ã€å…³é”®ã€‘å¿…é¡»ä¸ train.py ä¸­çš„ SYSTEM_PROMPT å®Œå…¨ä¸€è‡´
SYSTEM_PROMPT = "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è®¡ç®—æœºä¸“ä¸šé¢è¯•å®˜ï¼Œé£æ ¼ä¸¥è°¨ï¼Œå–œæ¬¢è¿½é—®åº•å±‚åŸç†ã€‚è¯·æ ¹æ®å€™é€‰äººçš„å›ç­”è¿›è¡Œè¿½é—®æˆ–ç‚¹è¯„ã€‚é¢è¯•ä¸­å¯¹è¯ä¸è¶…è¿‡10è½®ï¼Œå®Œæˆé¢è¯•æ—¶é¢è¯•å®˜ä¸»åŠ¨ç»“æŸå¹¶ç»™å‡ºæ‰“åˆ†å’Œç‚¹è¯„ã€‚"
# ===============================================================

def get_latest_checkpoint(output_dir):
    """è‡ªåŠ¨æŸ¥æ‰¾ output ç›®å½•ä¸‹æ•°å­—æœ€å¤§çš„ checkpoint æ–‡ä»¶å¤¹"""
    if not os.path.exists(output_dir):
        return None
    
    checkpoints = []
    for d in os.listdir(output_dir):
        if d.startswith("checkpoint-"):
            try:
                num = int(d.split("-")[-1])
                checkpoints.append((num, os.path.join(output_dir, d)))
            except ValueError:
                continue
    
    if not checkpoints:
        return None
    
    checkpoints.sort(key=lambda x: x[0])
    return checkpoints[-1][1]

def load_model():
    print("â³ æ­£åœ¨å¯»æ‰¾æœ€ä½³å¾®è°ƒæƒé‡...")
    lora_path = get_latest_checkpoint(OUTPUT_DIR)
    
    if not lora_path:
        print(f"âŒ é”™è¯¯ï¼šåœ¨ {OUTPUT_DIR} ä¸‹æ²¡æœ‰æ‰¾åˆ° checkpoint æ–‡ä»¶å¤¹ï¼è¯·ç¡®è®¤è®­ç»ƒæ˜¯å¦æˆåŠŸå®Œæˆã€‚")
        exit()
    print(f"âœ… æ‰¾åˆ°æœ€æ–°æƒé‡: {lora_path}")

    print("â³ æ­£åœ¨å®šä½æœ¬åœ°åŸºç¡€æ¨¡å‹ (ModelScope)...")
    try:
        # ä½¿ç”¨ snapshot_download è·å–æœ¬åœ°ç»å¯¹è·¯å¾„ï¼Œé˜²æ­¢é‡æ–°ä¸‹è½½
        model_dir = snapshot_download(MODEL_ID, cache_dir=MODEL_CACHE_DIR, revision="master")
        print(f"âœ… æœ¬åœ°åŸºç¡€æ¨¡å‹è·¯å¾„: {model_dir}")
    except Exception as e:
        print(f"âŒ å®šä½åŸºç¡€æ¨¡å‹å¤±è´¥: {e}")
        print(f"è¯·æ£€æŸ¥ç›®å½• {MODEL_CACHE_DIR} æ˜¯å¦å­˜åœ¨æ¨¡å‹æ–‡ä»¶ã€‚")
        exit()

    print("â³ æ­£åœ¨åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(lora_path, trust_remote_code=True)
    except:
        tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)

    model = AutoModelForCausalLM.from_pretrained(
        model_dir, 
        device_map="auto",
        torch_dtype=torch.bfloat16,
        trust_remote_code=True
    )

    print("â³ æ­£åœ¨æŒ‚è½½ LoRA å¾®è°ƒæƒé‡...")
    model = PeftModel.from_pretrained(model, lora_path)
    model.eval()
    
    print("ğŸ‰ é¢è¯•å®˜æ¨¡å‹åŠ è½½å®Œæˆï¼")
    return model, tokenizer

def main():
    model, tokenizer = load_model()
    
    # åˆå§‹åŒ–å†å²è®°å½•
    history = [
        {"role": "system", "content": SYSTEM_PROMPT}
    ]

    print("\n" + "="*60)
    print("ğŸ¤– AI é¢è¯•å®˜å·²å°±ä½ã€‚")
    print("ğŸ’¡ æŒ‡ä»¤ï¼šè¾“å…¥ 'exit' é€€å‡ºï¼Œè¾“å…¥ 'clear' æ¸…ç©ºè®°å½•é‡æ–°å¼€å§‹ã€‚")
    print("="*60 + "\n")

    # ã€æ³¨æ„ã€‘è¿™é‡Œä¸å†æŠ›å‡ºé¢„è®¾çš„å¼€åœºç™½ï¼Œç›´æ¥è¿›å…¥å¾ªç¯ç­‰å¾…ç”¨æˆ·è¾“å…¥
    
    while True:
        try:
            # è¿™é‡Œçš„ input æç¤ºç¬¦å¯ä»¥ç®€å•ç‚¹ï¼Œæˆ–è€…ç•™ç©º
            query = input("\nCandidate (ä½ ): ")
        except UnicodeDecodeError:
            print("âŒ è¾“å…¥ç¼–ç é”™è¯¯ï¼Œè¯·é‡è¯•")
            continue

        if query.strip() == "":
            continue
        if query.lower() in ["exit", "quit"]:
            print("ğŸ‘‹ é¢è¯•ç»“æŸã€‚")
            break
        if query.lower() == "clear":
            history = [{"role": "system", "content": SYSTEM_PROMPT}]
            os.system('cls' if os.name == 'nt' else 'clear')
            print("ğŸ”„ é¢è¯•å·²é‡ç½®ã€‚")
            continue

        # åŠ å…¥ç”¨æˆ·è¾“å…¥
        history.append({"role": "user", "content": query})

        # æ„å»º Prompt
        text = tokenizer.apply_chat_template(
            history,
            tokenize=False,
            add_generation_prompt=True
        )
        
        model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

        streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
        generation_kwargs = dict(
            model_inputs,
            streamer=streamer,
            max_new_tokens=512,
            temperature=0.7, 
            top_p=0.9,
            repetition_penalty=1.1 
        )

        thread = Thread(target=model.generate, kwargs=generation_kwargs)
        thread.start()

        print("Interviewer (AI): ", end="", flush=True)
        response_text = ""
        for new_text in streamer:
            print(new_text, end="", flush=True)
            response_text += new_text
        print("") 

        history.append({"role": "assistant", "content": response_text})

if __name__ == "__main__":
    main()